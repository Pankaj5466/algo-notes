
\chapter{Web Crawler}

\qs{What is a web crawler?}
    A web crawler is a system that explores the internet with specific goal in mind. The goal can be be something like. (aka applicatio of web crawler)
    \ls
        \i Generate a snapshot of Internet at current time.
        \i Browse the internet for code duplicacy / piracy.
        \i Browse the web for search index generation for gians like Google Search.
        \i  Copyright Violation finding on document shared on internet.
        \i Research finding of something related to some search.
        \i download  all humans-dog interaction pics from instagram for ML modal training with the captions for machine modal traning.
    \le
\qe

\q{List Some Applications of web crawler.}


\ta{Design Considerations}
Now as we know what the system does, lets finilize what all feature we are required to implement from this system. Also, with what performace (latency, availibity, consistency) is expected from our system.

\lstart
    \i How many page download per month are we expecting? 
    \r{let's suppose it 1B per month.}
    
    \i Are we desiging for a single website (like school webcrawler to see suspicision activtiy from each user profile) or we are planning for whole web?
    \r{We are planning for whole web.}

    \i What is the document we are designing our crawler for?
    \r{For now lets just say we need HTML, but our system should be flexible to extend for other type also with minimum changes.}

    \i For how long do we need the downloaded data to be kept?
    \r{For our business purpose we need to store at maximum 5 years.}

    \i How about handling addition/deletion of a webpage?
    \r{You should consider newly added and edited pages too, you can ignore the page beign deleted.}

    \i Duplicate Content? How does the crawler respose to duplicate content?\marginnote{\small{As per data 23percentage content on internet is duplicat}}
    \r{Just ignore them.}

    Above are some of the sample question you can ask to clarify the system behaviour and working. This is a good time to be on same level as you and interview as what is expected from you to desing.
\lend

Besides above, there are many nonfunctional requirement of every system. They can vary from system to system (for example banking system need to be highly consistency, but that is not needed for chat server or photo storage, see \ref{chapter:cap} for more detials )

Going through types and problems of a distirbuted system, we find that our crawler system needs to be:

\ls
\i Highly Scalabile : scalability is required to handle extreme parallalization to handle billions of web pages.

Beside above this sytem also have extra non-functional requirement, which can be named as:

\i Robustness : The web is full of trap. Bad HTML, unresponsive server, malicions linke tec. The crawler must handle all these cases well without impacting the system

\i Politines: The crawler should not make too many requirest to same host within short time interval

\i Extensibility: our system should be flexible enough if we required to support other file type.
\le

\ta{Estimations}

\ta{HLD Proposal}
Now, as we know what the system is and what features we are expected to implmement. Now its time to propose a high-level-desing of our solution.

We will start from some seed URL. We will also have a system that decies which URL to process next(we will call it URL Frontier). Once the URL is finilized, we will pass it for URL downloader. Once the HTML is downloaded, we need to parse it for desired data( the desired data can be image,video,html link,search words etc). Once parsed, we will check if this data/content has already appeared or not. If not, then we will go to next steps.

In the next step, we will extract all the link from the content. (If later we require to download images, this is where we will add additional module to download them). Then we will pass it to URL Filter. If the URL is already seen then it is dropped, else it is passed as biased parameter to URL Frontier. Which the restart the same process with different URL. 

Few things to keep in mind when desigin with above steps:
\begin{lstlisting}

    - URL Frontier is the main component which is responsible for next URL determination.
\end{lstlisting}