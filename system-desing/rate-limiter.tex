\chapter{Rate Limiter}

\ha{What is Rate Limter?}
A rate limiter is s system that limits the number of request that can be made to a system during a fixed time interval. It is used to controll incoming traffic or outgoing traffic. Server implement rate limiter to avoid overload of their system and avoid DDoS attack and better user experiance for majority of users.

In HTTP world, a rate limiter limits the number of client request allowed to be sent over a specified period. If the API request count exceeds the threshold defined by the rate limiter, all the excess calls are blocked.

\hb{Real World use case of rate limiting system.}
\lstart
    \i A user can write no more than 2 post per second.
    \i You can create a maximum of 10 accounts per day from same IP address.
    \i You can claim reward no more than 5 times per week from same device.
    \i Prevent resource starvation causes by DDoS attack.
    \i Reduce cost and cost approximation by reducing allowed request per minute.
    \i Allocating different API different priority  by assigining uneven timing to them.
\lend



\ha{Design Scope Discussion}

List of query that need to be finilized before starting the design.

\lstart
    \i It's a client side rate limiting or server side?
    \r Server Side.

    \i What are the parameter upon which the rate limiter will throttle the request? Is it IP address, userID or other properties?
    \r Let's say we want it to flexible.

    \i Number of calls to the Rate limiter?
    \r Lets say its 50M calls to the server upon which we want to intergrate the rate limiter.

    \i Is the system distriburted?
    \r Of Course.

    \i Do we need to inform the user that their request are beign throttled/dropped?
    \r Yes, a good system must not remain unambigous. We need to return 429 HTTP code in case the request get blocked due to rate limiting throttling. 
\lend

\hb{Non Funcitonal Requerement }
Here are some of the non functional requirement that we need to consider while desiging our system.

\lstart
    \i The system must not introduce significient latency. i.e it should be a \u{low latency} system, and should not slow down the HTTP request.
    
    \i Use minimum memory.
    \i  Exception Handling: return appropriate error-code upon different state.

    \i High Fault tolerance: If there is any problem with the rate limiter(ex: a cache servers goes offline), it should not affect the entire system.
\lend

\ha{Algorithms For Rate Limiting}

Where should the system be places?\\
It can be placed at API gateway or standalone version in between client and API server or can be buit inside of API server.

Rate Limting Algorithms.
\ls
    \i Token Bucket Algorithm.
    \i Leaking Bucket
    \i Fixed Widnow Counter
    \i Sliding Window long
    \i Sliding Window Counter
\le

Explain each of the above algorithms. In case of any query pleaser refer the book.


\ha{HLD Proposal}
Now as we understand the basic idea of rate limiting. It need to maintains a counter variable, and if the counter is above threshold then the subsequent request will be dropped. 

We need to pay attention that we are desigining a distributed system, so the counter will be incremented in a distributed way. 

Also the counter cannot be stored in disk as it will slow down the operation. So we will use in-memory data-store like redis, which is fast and provide distributed INCREMENT and EXPIRE operations. 
\marginnote{How redis prevent data-loss during system crash?}

\begin{lfigure}{resources/rate-limiter-hld.png}{0.5}{0.4}
    Rate limiter middleware is provided in between Client and API server. For every request, its corresponding count is checked from redis. If the count is above threshold then the request is dropped else it is allowed to pass to API server.
\end{lfigure}

\ha{LLD | Desing Deep Dive}
    In the desing deep dive, we will analyse the HLD for possible improvement and possible paint point which we must remove for a better system.

    \hb{ Distributed Rate Limiter | Solving Race Condition with minimum time:}
    As we know in a distributed system, the counter can be updated by tho threads in parallel. To avoid the race condition, one obivious solution is to use lock on the counter variable. \b{But it will slow down the system.}
    \rfl{Redis achieves atomicity by using a single-threaded architecture}
    In here, single type of operation is performed on single thread to avoid race condition entirely.

    \hb{Rate Limiting Rules(Storage,Actions):}
    We haven't answersed quesitions like:\\
     (a) How and rate limiting rules created and where are they stored?\\
     (b) How to handle request that are rate limited?

    (a) For First we can have a JSON structure that defines the rate limiter configuration. Its structure can be something like.
    \begin{code3}
        {
            domain:"messaing|auth",
            descriptors:
            {
                key:"message_type",
                value:"marketing",
                rate_limit_s:250s,
                request_per_unit:5
            }
        }
    \end{code3}

    This set of JSON will be stored in disk, and some worker thread can upload it to cache for fast access.

    (b) For second, we can add addition headers to the client response that descibes the system in full.
    \begin{code3}
        - X-Rate Limit-Remaining: The Remaning number of allowed request within the window.
        - X-Ratelimit-Limit
        - X-RateLimit-Retry-After
    \end{code3}

    
    \hb{Synchronization Issue}
    Again this problem will not arose in a single-thread architecture like redis.

   \begin{lfigure}{resources/rate-limiter-lld.png}{0.7}{0.28}
    Worker thread retrives the rules from disk and sotre them in cache. (cold cache?)
    
    Request beign dropped can be queue or dropped.
   \end{lfigure}

    \hb{Performance Optimization}
    Performance Optimization is common amoung distributed system. Some of the ways for optimizatioin for a distriburted system are: CDN, Cache, Multiple Data Center, Faster Algoritms, Alternater Desing, Decouple video decoupling via event queue, batch processing, precomputation for faster run-time tradeoff.
    
    In Rate limiter case, we must provide multiple-data-center, else request from far away user can reach a large time to reach the rate-limiter system itself. One best approach use it to provide rate-limiter to region at which the API server itself are placed.

    Along with it, during a sudden spike how can you change you alorithm that ensure that less number of site user experience system outage?

    Should we support a hard-limit or soft-limit? In soft limit the number of request are allowed to vary by say 10per.

    \hb{Monitoring}
    Monitoring metrix is must for fine tuning the rate limiter bandwidth. We can track QPS and other different factor, and we can take decision based on them.

    For example if rate limiting rules are too strict then relax them a bit. (rate limiting are too strict can be identified by the number of request being dropped per day.)
    
\ha{Further Discussion Points}
\ls
    \i Avoid Beign Rate Limited.
\le


\ha{Wrap Up}

Rate limiter system is good problem that shows how you can desing a simple system and intergrate with existing system. Rate limiter also show how to chose between different atomicity levels and Snapshot Isolation Levels.
\marginnote{See Data Intensive book for Atomicity Level and Isolation Levels}

Multple data center is must for such type of system. Algorithm selection and explaining their working is also a critical part of this system.

\todo{Summarize the chapter.}